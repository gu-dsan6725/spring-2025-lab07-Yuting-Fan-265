{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune `meta-llama/Meta-Llama-3-8B-Instruct` on an EC2 instance using `Unsloth`\n",
    "---\n",
    "\n",
    "Unsloth makes finetuning large language models like Llama-3, Mistral, Phi-4 and Gemma 2x faster, use 70% less memory, and with no degradation in accuracy!\n",
    "\n",
    "**Note**: ***This notebook is run on a `g6e.12xlarge` instance. Follow the prerequisite steps [here](README.md)***\n",
    "\n",
    "In this example, we will be fine tuning the llama3 8b instruct model. There are several 4bit pre quantized models that `unsloth` provides that are not gated. This supports 4x faster downloading with no OOMs. In this case, we will be using the standard `meta-llama/Meta-Llama-3-8B-Instruct` model from hugging face. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-13 17:33:26,279\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "import globals as g\n",
    "from dotenv import load_dotenv\n",
    "from unsloth import to_sharegpt\n",
    "from datasets import load_dataset\n",
    "from unsloth import FastLanguageModel\n",
    "from unsloth import standardize_sharegpt\n",
    "from ec2_metrics import EC2MetricsCallback\n",
    "\n",
    "# Create a logger\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "# Remove existing handlers\n",
    "logger.handlers.clear()\n",
    "\n",
    "# Add a simple handler\n",
    "handler = logging.StreamHandler()\n",
    "formatter = logging.Formatter('[%(asctime)s] p%(process)s {%(filename)s:%(lineno)d} %(levelname)s - %(message)s')\n",
    "handler.setFormatter(formatter)\n",
    "logger.addHandler(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-13 17:33:31,296] p14585 {2478216038.py:12} INFO - hf_model_id=meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "import getpass\n",
    "load_dotenv()\n",
    "if not os.getenv(\"HF_TOKEN\"):\n",
    "    os.environ[\"HF_TOKEN\"] = getpass.getpass(\"Enter your HuggingFace token: \")\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "if not os.getenv(\"HF_MODEL_ID\"):\n",
    "    hf_model_id  = input(\"Enter the model id to use for fine-tuning (e.g. meta-llama/Llama-3.1-8B-Instruct): \")\n",
    "else:\n",
    "    hf_model_id = os.getenv(\"HF_MODEL_ID\")\n",
    "logger.info(f\"hf_model_id={hf_model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = False # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "DATASET_OF_INTEREST: str = 'vicgalle/alpaca-gpt4'\n",
    "\n",
    "ALPACA_PROMPT: str = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "{}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.49.0.\n",
      "   \\\\   /|    GPU: NVIDIA L4. Max memory: 22.045 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [02:02<00:00, 30.54s/it]\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "        model_name = hf_model_id,\n",
    "        max_seq_length = max_seq_length,\n",
    "        dtype = dtype,\n",
    "        load_in_4bit = load_in_4bit,\n",
    "        token = hf_token # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    "    )\n",
    "except Exception as e:\n",
    "    logger.error(f\"Error occurred while loading the model: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 16,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep\n",
    "\n",
    "We now use the Alpaca dataset from vicgalle, which is a version of 52K of the original Alpaca dataset generated from GPT4. You can replace this code section with your own data prep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-13 17:36:17,244] p14585 {2022229014.py:2} INFO - Columns in the dataset: ['instruction', 'input', 'output', 'text']\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(DATASET_OF_INTEREST, split=\"train\")\n",
    "logger.info(f\"Columns in the dataset: {dataset.column_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = to_sharegpt(\n",
    "    dataset,\n",
    "    merged_prompt=\"{instruction}[[\\nYour input is:\\n{input}]]\",\n",
    "    output_column_name=\"output\",\n",
    "    conversation_extension=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the standardize_sharegpt function to just make the dataset in a correct format for finetuning\n",
    "dataset = standardize_sharegpt(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversations': [[{'content': 'Give three tips for staying healthy.',\n",
      "                     'role': 'user'},\n",
      "                    {'content': '1. Eat a balanced and nutritious diet: Make '\n",
      "                                'sure your meals are inclusive of a variety of '\n",
      "                                'fruits and vegetables, lean protein, whole '\n",
      "                                'grains, and healthy fats. This helps to '\n",
      "                                'provide your body with the essential '\n",
      "                                'nutrients to function at its best and can '\n",
      "                                'help prevent chronic diseases.\\n'\n",
      "                                '\\n'\n",
      "                                '2. Engage in regular physical activity: '\n",
      "                                'Exercise is crucial for maintaining strong '\n",
      "                                'bones, muscles, and cardiovascular health. '\n",
      "                                'Aim for at least 150 minutes of moderate '\n",
      "                                'aerobic exercise or 75 minutes of vigorous '\n",
      "                                'exercise each week.\\n'\n",
      "                                '\\n'\n",
      "                                '3. Get enough sleep: Getting enough quality '\n",
      "                                'sleep is crucial for physical and mental '\n",
      "                                'well-being. It helps to regulate mood, '\n",
      "                                'improve cognitive function, and supports '\n",
      "                                'healthy growth and immune function. Aim for '\n",
      "                                '7-9 hours of sleep each night.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Describe what a monotheistic religion is.',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'A monotheistic religion is a type of religion '\n",
      "                                'that believes in the existence of only one '\n",
      "                                'supreme and all-powerful deity, who is '\n",
      "                                'considered the creator and ruler of the '\n",
      "                                'universe. This deity is worshiped as the '\n",
      "                                'ultimate and only divine being, and followers '\n",
      "                                'of such religions often see their deity as '\n",
      "                                'omniscient, omnipotent, and omnibenevolent. '\n",
      "                                'Some of the most widely practiced '\n",
      "                                'monotheistic religions in the world today '\n",
      "                                'include Christianity, Islam, and Judaism, '\n",
      "                                'among others. The concept of monotheism '\n",
      "                                'differs from polytheism, which believes in '\n",
      "                                'the existence of multiple gods, and from '\n",
      "                                'atheism, which denies the existence of any '\n",
      "                                'deity.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'How does one add a chart to a document?',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'To add a chart to a document, follow these '\n",
      "                                'steps:\\n'\n",
      "                                '\\n'\n",
      "                                '1. Open the document where you want to insert '\n",
      "                                'the chart.\\n'\n",
      "                                '2. Click the location where you want to '\n",
      "                                'insert the chart.\\n'\n",
      "                                '3. In most word processors, you can go to the '\n",
      "                                '**Insert** tab, where you can find a '\n",
      "                                '**Chart** option. Click on it.\\n'\n",
      "                                \"4. A new window will appear, and you'll be \"\n",
      "                                'able to select the chart type that you want '\n",
      "                                'to use (column, pie, line, bar, area, '\n",
      "                                'scatter, etc.).\\n'\n",
      "                                \"5. Once you've selected your chart type, \"\n",
      "                                'youâ€™ll be prompted to enter your data into a '\n",
      "                                'spreadsheet. You can either type your data in '\n",
      "                                'manually or copy it from an existing data '\n",
      "                                'source.\\n'\n",
      "                                '6. Edit your chart data and customize its '\n",
      "                                \"appearance to fit your document's style or \"\n",
      "                                'branding.\\n'\n",
      "                                '7. When youâ€™re happy with how the chart '\n",
      "                                'looks, click \"OK\" or \"Insert\" to add the '\n",
      "                                'chart to your document.\\n'\n",
      "                                '8. Optionally, you can add a chart title or '\n",
      "                                'labels to the axes to make the chart easier '\n",
      "                                'to understand.\\n'\n",
      "                                '\\n'\n",
      "                                'These instructions may vary depending on the '\n",
      "                                'type of word processor you are using.',\n",
      "                     'role': 'assistant'}],\n",
      "                   [{'content': 'What are the three primary colors?',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'The three primary colors are red, blue, and '\n",
      "                                'yellow. These colors are called primary '\n",
      "                                'because they cannot be created by mixing '\n",
      "                                'other colors and all other colors can be made '\n",
      "                                'by combining them in various proportions. In '\n",
      "                                'the additive color system, used for light, '\n",
      "                                'the primary colors are red, green, and blue '\n",
      "                                '(RGB).',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Identify 3 different features of the given '\n",
      "                                'product.\\n'\n",
      "                                'Your input is:\\n'\n",
      "                                'Apple Watch Series 6',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'Three features of the Apple Watch Series 6 '\n",
      "                                'are:\\n'\n",
      "                                '\\n'\n",
      "                                '1. Blood Oxygen Monitoring: The Apple Watch '\n",
      "                                'Series 6 includes a Blood Oxygen sensor that '\n",
      "                                'measures the oxygen saturation (SpO2) of the '\n",
      "                                'wearerâ€™s blood. It uses infrared and red LEDs '\n",
      "                                'to take measurements, alongside photodiodes '\n",
      "                                'that measure the light reflected from the '\n",
      "                                \"blood in the user's veins.\\n\"\n",
      "                                '\\n'\n",
      "                                '2. Always-On Retina Display: The Always-On '\n",
      "                                'Retina display ensures that the watch face is '\n",
      "                                'always visible, even when the userâ€™s wrist is '\n",
      "                                'down. This makes it easier to check the time, '\n",
      "                                'notifications, and other important '\n",
      "                                'information without having to raise your '\n",
      "                                'wrist or tap on the screen.\\n'\n",
      "                                '\\n'\n",
      "                                '3. Fitness Tracking: The Apple Watch Series 6 '\n",
      "                                'has a range of features designed to help '\n",
      "                                'users track their fitness and stay active. '\n",
      "                                'These include tracking workouts, monitoring '\n",
      "                                'daily activity levels, and setting reminders '\n",
      "                                'to stand or move around periodically. The '\n",
      "                                'watch also includes a heart rate monitor to '\n",
      "                                'track cardio fitness levels during exercise.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Given a description of a character, come up '\n",
      "                                \"with possible motivations for the character's \"\n",
      "                                'behaviour.\\n'\n",
      "                                'Your input is:\\n'\n",
      "                                'The character is a young man who is often '\n",
      "                                'confrontational and rebellious.',\n",
      "                     'role': 'user'},\n",
      "                    {'content': '1. The young man may have grown up in a '\n",
      "                                'challenging home environment where he had to '\n",
      "                                'fight to be heard, leading him to develop '\n",
      "                                'confrontational behaviors.\\n'\n",
      "                                '\\n'\n",
      "                                '2. He may be struggling with feelings of '\n",
      "                                'insecurity or low self-worth, leading him to '\n",
      "                                'act out in rebellious ways to gain attention '\n",
      "                                'or assert his individuality.\\n'\n",
      "                                '\\n'\n",
      "                                '3. The young man may have experienced a '\n",
      "                                'traumatic event or loss, causing him to feel '\n",
      "                                'anger and resentment towards the world, '\n",
      "                                'leading him to behave confrontationally.\\n'\n",
      "                                '\\n'\n",
      "                                '4. He may feel misunderstood or stifled by '\n",
      "                                'societal expectations, leading him to rebel '\n",
      "                                'against norms and conventions as a means of '\n",
      "                                'expressing his frustration.\\n'\n",
      "                                '\\n'\n",
      "                                '5. The young man may be searching for a sense '\n",
      "                                'of identity or purpose, causing him to engage '\n",
      "                                'in confrontational behavior as a way of '\n",
      "                                'defining and asserting himself.\\n'\n",
      "                                '\\n'\n",
      "                                '6. He may be influenced by a group of peers '\n",
      "                                'who engage in rebellious behavior, causing '\n",
      "                                'him to adopt similar behaviors in order to '\n",
      "                                'fit in with the group.',\n",
      "                     'role': 'assistant'}],\n",
      "                   [{'content': 'Describe the structure of an atom.',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'An atom is the basic building block of all '\n",
      "                                'matter and is made up of three types of '\n",
      "                                'particles: protons, neutrons, and electrons. '\n",
      "                                'The structure of an atom can be described as '\n",
      "                                'a nucleus at the center surrounded by a cloud '\n",
      "                                'of electrons.\\n'\n",
      "                                '\\n'\n",
      "                                'The nucleus of an atom is made up of protons '\n",
      "                                'and neutrons. Protons are positively charged '\n",
      "                                'particles and neutrons are neutral particles '\n",
      "                                'with no charge. Both of these particles are '\n",
      "                                'located in the nucleus of the atom, which is '\n",
      "                                'at the center of the atom and contains most '\n",
      "                                \"of the atom's mass.\\n\"\n",
      "                                '\\n'\n",
      "                                'Surrounding the nucleus of the atom is a '\n",
      "                                'cloud of electrons. Electrons are negatively '\n",
      "                                'charged particles that are in constant motion '\n",
      "                                'around the nucleus. The electron cloud is '\n",
      "                                'divided into shells or orbitals, and each '\n",
      "                                'shell can hold a certain number of electrons. '\n",
      "                                'The number of electrons in the outermost '\n",
      "                                'shell, called the valence shell, determines '\n",
      "                                'the chemical properties of the atom. \\n'\n",
      "                                '\\n'\n",
      "                                'In a neutral atom, the number of protons in '\n",
      "                                'the nucleus is equal to the number of '\n",
      "                                'electrons in the electron cloud, so the '\n",
      "                                'positive and negative charges balance out and '\n",
      "                                'the atom has no overall charge. The number of '\n",
      "                                'protons, also called the atomic number, '\n",
      "                                'determines what element the atom is.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Describe the setting of the book \"Alice in '\n",
      "                                'Wonderland\".',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'The setting of the book \"Alice in Wonderland\" '\n",
      "                                'is primarily in a fantasy world located in a '\n",
      "                                'rabbit hole that Alice, the main character, '\n",
      "                                'falls into. This surreal world is called '\n",
      "                                'Wonderland and is filled with strange '\n",
      "                                'anthropomorphic creatures, magical '\n",
      "                                'occurrences, and illogical events. The '\n",
      "                                'environment changes from one scene to '\n",
      "                                'another, including lush gardens, forests, and '\n",
      "                                'the royal courtyard of the Queen of Hearts. '\n",
      "                                'As Alice moves through the different parts of '\n",
      "                                'Wonderland, she encounters unusual and '\n",
      "                                'puzzling situations, like changing in size, '\n",
      "                                'attending an unusual tea party, and partaking '\n",
      "                                'in a nonsensical trial. The setting of the '\n",
      "                                'book is dream-like and whimsical, with a '\n",
      "                                'touch of illogic and absurdity.',\n",
      "                     'role': 'assistant'},\n",
      "                    {'content': 'Rearrange the words given in the input to '\n",
      "                                'make a meaningful sentence.\\n'\n",
      "                                'Your input is:\\n'\n",
      "                                'society a modern in work importance gender of',\n",
      "                     'role': 'user'},\n",
      "                    {'content': 'The importance of gender in modern society '\n",
      "                                'work.',\n",
      "                     'role': 'assistant'}]]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(dataset[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth: We automatically added an EOS token to stop endless generations.\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:04<00:00, 12218.18 examples/s]\n"
     ]
    }
   ],
   "source": [
    "chat_template = \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.\n",
    "\n",
    "### Instruction:\n",
    "{INPUT}\n",
    "\n",
    "### Response:\n",
    "{OUTPUT}\"\"\"\n",
    "\n",
    "from unsloth import apply_chat_template\n",
    "\n",
    "dataset = apply_chat_template(\n",
    "    dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    chat_template=chat_template,\n",
    "    # default_system_message = \"You are a helpful assistant\", << [OPTIONAL]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:03<00:00, 16572.66 examples/s]\n",
      "Applying chat template to train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:04<00:00, 12820.12 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:30<00:00, 1690.67 examples/s]\n",
      "Tokenizing train dataset (num_proc=2): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 52002/52002 [00:12<00:00, 4125.90 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.34 s, sys: 523 ms, total: 1.86 s\n",
      "Wall time: 51.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train the model\n",
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from unsloth import is_bfloat16_supported\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = dataset,\n",
    "    dataset_text_field = \"text\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dataset_num_proc = 2,\n",
    "    packing = False, # Can make training 5x faster for short sequences.\n",
    "    args = TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 60,\n",
    "        # num_train_epochs = 1, # For longer training runs!\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not is_bfloat16_supported(),\n",
    "        bf16 = is_bfloat16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "        report_to = \"none\", # Use this for WandB etc\n",
    "    ),\n",
    "    callbacks=[EC2MetricsCallback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
      "   \\\\   /|    Num examples = 52,002 | Num Epochs = 1\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
      "\\        /    Total batch size = 8 | Total steps = 60\n",
      " \"-____-\"     Number of trainable parameters = 41,943,040\n",
      "[2025-03-13 17:38:24,472] p14585 {ec2_metrics.py:184} INFO - Training started. Initiating EC2 metrics collection.\n",
      "[2025-03-13 17:38:24,473] p14585 {ec2_metrics.py:170} INFO - Writing header: ['timestamp', 'cpu_percent_mean', 'memory_percent_mean', 'memory_used_mean', 'gpu_utilization_mean', 'gpu_memory_used_mean', 'gpu_memory_free_mean', 'gpu_memory_total_mean']\n",
      "[2025-03-13 17:38:24,474] p14585 {ec2_metrics.py:41} INFO - Starting collection\n",
      "[2025-03-13 17:38:24,851] p14585 {ec2_metrics.py:143} INFO - Starting daemon collector to run in background\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 05:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.446100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.435900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.390900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.485800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.568900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.311700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.336400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.326300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.286300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.089700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.138400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.130000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.073700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.072600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.214300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.130500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.083000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.053600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.106500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.979300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.066000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.047600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.077900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>1.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>1.081600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>1.069500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.147900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>1.095800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>1.087800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.985400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>1.076400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>1.158800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>1.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>1.048500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>1.020300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.986700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.021200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>1.033900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>1.065800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>1.111800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>1.007700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>1.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>1.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>1.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>1.043600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.216100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>1.095300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>1.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>1.075300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.992300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>1.008800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>1.052100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>1.008500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>1.035500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.996900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.024000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-13 17:44:00,617] p14585 {ec2_metrics.py:191} INFO - Training ended. Stopping EC2 metrics collection.\n",
      "[2025-03-13 17:44:00,618] p14585 {ec2_metrics.py:33} INFO - Stopped collection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 8s, sys: 2min 30s, total: 5min 38s\n",
      "Wall time: 5min 37s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-03-13 17:44:04,902] p14585 {ec2_metrics.py:33} INFO - Stopped collection\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# this will initiate the training process and also log the EC2 utilization metrics, such as the GPU\n",
    "# utilization, CPU utilization, etc.\n",
    "trainer_stats = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log the trainer stats\n",
    "---\n",
    "\n",
    "In this step, we log some of the trainer stats, such as the number of global steps it took to get to a specific training loss, the train runtime, samples per second, steps per second, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the training stats in a readable way\n",
    "output_text = f\"\"\"Training Statistics:\n",
    "Global Steps: {trainer_stats.global_step}\n",
    "Training Loss: {trainer_stats.training_loss:.4f}\n",
    "\n",
    "Metrics:\n",
    "- Train Runtime: {trainer_stats.metrics['train_runtime']:.3f} seconds\n",
    "- Training Samples/Second: {trainer_stats.metrics['train_samples_per_second']:.3f}\n",
    "- Training Steps/Second: {trainer_stats.metrics['train_steps_per_second']:.3f}\n",
    "- Total FLOPS: {trainer_stats.metrics['total_flos']:.2e}\n",
    "- Final Train Loss: {trainer_stats.metrics['train_loss']:.4f}\n",
    "\"\"\"\n",
    "\n",
    "# Save to a text file\n",
    "with open(os.path.join(g.RESULTS_DIR, g.TRAINING_STATS), 'w') as f:\n",
    "    f.write(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The next number in the Fibonacci sequence is 13.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "messages = [                    # Change below!\n",
    "    {\"role\": \"user\", \"content\": \"Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8,\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tallest tower in France is called the Eiffel Tower.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
    "messages = [                         # Change below!\n",
    "    {\"role\": \"user\",      \"content\": \"Continue the fibonacci sequence! Your input is 1, 1, 2, 3, 5, 8\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"The fibonacci sequence continues as 13, 21, 34, 55 and 89.\"},\n",
    "    {\"role\": \"user\",      \"content\": \"What is France's tallest tower called?\"},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True,\n",
    "    return_tensors = \"pt\",\n",
    ").to(\"cuda\")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
    "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('lora_model/tokenizer_config.json',\n",
       " 'lora_model/special_tokens_map.json',\n",
       " 'lora_model/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the model\n",
    "model.save_pretrained(\"lora_model\")  # Local saving\n",
    "tokenizer.save_pretrained(\"lora_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ollama Support [Optional]\n",
    "\n",
    "Unsloth now allows you to automatically finetune and create a Modelfile, and export to Ollama! This makes finetuning much easier and provides a seamless workflow from Unsloth to Ollama!\n",
    "\n",
    "Let's first install Ollama!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Cleaning up old version at /usr/local/lib/ollama\n",
      ">>> Installing ollama to /usr/local\n",
      ">>> Downloading Linux amd64 bundle\n",
      "######################################################################## 100.0%####################                                                36.6%#############################################               83.3%\n",
      ">>> Adding ollama user to render group...\n",
      ">>> Adding ollama user to video group...\n",
      ">>> Adding current user to ollama group...\n",
      ">>> Creating ollama systemd service...\n",
      ">>> Enabling and starting ollama service...\n",
      ">>> NVIDIA GPU installed.\n"
     ]
    }
   ],
   "source": [
    "!curl -fsSL https://ollama.com/install.sh | sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 6.42 out of 15.01 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/llama.cpp'\n",
      "make: Leaving directory '/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/llama.cpp'\n",
      "-- Warning: ccache not found - consider installing it for faster compilation or disable this warning with GGML_CCACHE=OFF\n",
      "-- CMAKE_SYSTEM_PROCESSOR: x86_64\n",
      "-- Including CPU backend\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Makefile:2: *** The Makefile build is deprecated. Use the CMake build instead. For more details, see https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md.  Stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- x86 detected\n",
      "-- Adding CPU backend variant ggml-cpu: -march=native \n",
      "-- Configuring done (0.2s)\n",
      "-- Generating done (0.3s)\n",
      "-- Build files have been written to: /home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/llama.cpp/build\n",
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 6.16 out of 15.01 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at inline_container.cc:603] . unexpected pos 576 vs 470",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 850\u001b[0m     _save(\n\u001b[1;32m    851\u001b[0m         obj,\n\u001b[1;32m    852\u001b[0m         opened_zipfile,\n\u001b[1;32m    853\u001b[0m         pickle_module,\n\u001b[1;32m    854\u001b[0m         pickle_protocol,\n\u001b[1;32m    855\u001b[0m         _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/serialization.py:1114\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m zip_file\u001b[39m.\u001b[39;49mwrite_record(name, storage, num_bytes)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/0: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/unsloth/save.py:1703\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1702\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1703\u001b[0m     new_save_directory, old_username \u001b[39m=\u001b[39m unsloth_save_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marguments)\n\u001b[1;32m   1704\u001b[0m     makefile \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/unsloth/save.py:587\u001b[0m, in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    586\u001b[0m filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(temporary_location, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 587\u001b[0m torch\u001b[39m.\u001b[39;49msave(W, filename, pickle_module \u001b[39m=\u001b[39;49m pickle, pickle_protocol \u001b[39m=\u001b[39;49m pickle\u001b[39m.\u001b[39;49mHIGHEST_PROTOCOL,)\n\u001b[1;32m    588\u001b[0m \u001b[39m# weights_only = True weirdly fails?\u001b[39;00m\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/serialization.py:690\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mwrite_end_of_file()\n\u001b[1;32m    691\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 576 vs 470",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/serialization.py:850\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m--> 850\u001b[0m     _save(\n\u001b[1;32m    851\u001b[0m         obj,\n\u001b[1;32m    852\u001b[0m         opened_zipfile,\n\u001b[1;32m    853\u001b[0m         pickle_module,\n\u001b[1;32m    854\u001b[0m         pickle_protocol,\n\u001b[1;32m    855\u001b[0m         _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m     )\n\u001b[1;32m    857\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/serialization.py:1114\u001b[0m, in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[39m# Now that it is on the CPU we can directly copy it into the zip file\u001b[39;00m\n\u001b[0;32m-> 1114\u001b[0m zip_file\u001b[39m.\u001b[39;49mwrite_record(name, storage, num_bytes)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:778] . PytorchStreamWriter failed writing file data/0: file write failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb Cell 22\u001b[0m line \u001b[0;36m2\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-18-212-133-228.compute-1.amazonaws.com/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Save to 8bit Q8_0\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2-18-212-133-228.compute-1.amazonaws.com/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m: model\u001b[39m.\u001b[39msave_pretrained_gguf(\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m, tokenizer,)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-18-212-133-228.compute-1.amazonaws.com/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Remember to go to https://huggingface.co/settings/tokens for a token!\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-18-212-133-228.compute-1.amazonaws.com/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# And change hf to your username!\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-18-212-133-228.compute-1.amazonaws.com/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb#X30sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mFalse\u001b[39;00m: model\u001b[39m.\u001b[39mpush_to_hub_gguf(\u001b[39m\"\u001b[39m\u001b[39mhf/model\u001b[39m\u001b[39m\"\u001b[39m, tokenizer, token \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/unsloth/save.py:1719\u001b[0m, in \u001b[0;36munsloth_save_pretrained_gguf\u001b[0;34m(self, save_directory, tokenizer, quantization_method, first_conversion, push_to_hub, token, private, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m   1717\u001b[0m     git_clone\u001b[39m.\u001b[39mwait()\n\u001b[1;32m   1718\u001b[0m     makefile \u001b[39m=\u001b[39m install_llama_cpp_make_non_blocking()\n\u001b[0;32m-> 1719\u001b[0m     new_save_directory, old_username \u001b[39m=\u001b[39m unsloth_save_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49marguments)\n\u001b[1;32m   1720\u001b[0m     python_install\u001b[39m.\u001b[39mwait()\n\u001b[1;32m   1721\u001b[0m \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/unsloth/save.py:587\u001b[0m, in \u001b[0;36munsloth_save_model\u001b[0;34m(model, tokenizer, save_directory, save_method, push_to_hub, token, is_main_process, state_dict, save_function, max_shard_size, safe_serialization, variant, save_peft_format, use_temp_dir, commit_message, private, create_pr, revision, commit_description, tags, temporary_location, maximum_memory_usage)\u001b[0m\n\u001b[1;32m    585\u001b[0m logger\u001b[39m.\u001b[39mwarning_once(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mWe will save to Disk and not RAM now.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    586\u001b[0m filename \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(temporary_location, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 587\u001b[0m torch\u001b[39m.\u001b[39;49msave(W, filename, pickle_module \u001b[39m=\u001b[39;49m pickle, pickle_protocol \u001b[39m=\u001b[39;49m pickle\u001b[39m.\u001b[39;49mHIGHEST_PROTOCOL,)\n\u001b[1;32m    588\u001b[0m \u001b[39m# weights_only = True weirdly fails?\u001b[39;00m\n\u001b[1;32m    589\u001b[0m state_dict[name] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(filename, map_location \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m, mmap \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m, weights_only \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/serialization.py:849\u001b[0m, in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[0m\n\u001b[1;32m    846\u001b[0m _check_save_filelike(f)\n\u001b[1;32m    848\u001b[0m \u001b[39mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m--> 849\u001b[0m     \u001b[39mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[39mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m    850\u001b[0m         _save(\n\u001b[1;32m    851\u001b[0m             obj,\n\u001b[1;32m    852\u001b[0m             opened_zipfile,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    855\u001b[0m             _disable_byteorder_record,\n\u001b[1;32m    856\u001b[0m         )\n\u001b[1;32m    857\u001b[0m         \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/jl3081/spring-2025-lab07-jtjt427/.venv/lib/python3.12/site-packages/torch/serialization.py:690\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    689\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 690\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfile_like\u001b[39m.\u001b[39;49mwrite_end_of_file()\n\u001b[1;32m    691\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfile_stream\u001b[39m.\u001b[39mclose()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:603] . unexpected pos 576 vs 470"
     ]
    }
   ],
   "source": [
    "# Save to 8bit Q8_0\n",
    "if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
    "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
    "# And change hf to your username!\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
    "\n",
    "# Save to 16bit GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
    "\n",
    "# Save to q4_k_m GGUF\n",
    "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
    "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
    "\n",
    "# Save to multiple GGUF options - much faster if you want multiple!\n",
    "if False:\n",
    "    model.push_to_hub_gguf(\n",
    "        \"hf/model\", # Change hf to your username!\n",
    "        tokenizer,\n",
    "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
    "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('database or disk is full')).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-18-212-133-228.compute-1.amazonaws.com/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m subprocess\u001b[39m.\u001b[39mPopen([\u001b[39m\"\u001b[39m\u001b[39mollama\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mserve\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bec2-18-212-133-228.compute-1.amazonaws.com/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtime\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bec2-18-212-133-228.compute-1.amazonaws.com/home/ubuntu/yf265/spring-2025-lab07-Yuting-Fan-265/SFT_hf_model_unsloth.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m time\u001b[39m.\u001b[39;49msleep(\u001b[39m3\u001b[39;49m) \n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: listen tcp 127.0.0.1:11434: bind: address already in use\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "subprocess.Popen([\"ollama\", \"serve\"])\n",
    "import time\n",
    "\n",
    "time.sleep(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ollama create unsloth_model -f ./model/Modelfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference against the model\n",
    "!curl http://localhost:11434/api/chat -d '{ \\\n",
    "    \"model\": \"unsloth_model\", \\\n",
    "    \"messages\": [ \\\n",
    "        { \"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\" } \\\n",
    "    ] \\\n",
    "    }'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run inference against the model\n",
    "import json\n",
    "result = subprocess.run(\n",
    "    [\n",
    "        \"curl\",\n",
    "        \"http://localhost:11434/api/generate\",\n",
    "        \"-d\",\n",
    "        '{\"model\": \"unsloth_model\", \"prompt\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\", \"stream\": false}',\n",
    "    ],\n",
    "    capture_output=True,\n",
    "    text=True,\n",
    ")\n",
    "\n",
    "response_data = json.loads(result.stdout)\n",
    "print(f\"Response generated: {response_data['response']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (uv env)",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
